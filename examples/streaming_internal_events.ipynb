{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6128a94",
   "metadata": {},
   "source": [
    "# Streaming Internal Events\n",
    "\n",
    "Event streaming is intrisinc to workflows and very easy to implement, as you can see in this example code:\n",
    "\n",
    "```python\n",
    "class SpecialEvent(Event):\n",
    "    pass\n",
    "\n",
    "class OtherEvent(Event):\n",
    "    pass\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    ...\n",
    "\n",
    "wf = MyWorkflow(...)\n",
    "\n",
    "handler = wf.run()\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, SpecialEvent):\n",
    "        print(\"This is a special event, hurray!\")\n",
    "    else:\n",
    "        print(\"Not a special event :(\")\n",
    "```\n",
    "\n",
    "Beyond streaming user-defined events, workflows can also stream internal events, such as changes in the state of the current step, input and output events, modifications of the workflow state and variation in the content of internal queues.\n",
    "\n",
    "In the following example, we will see how we can leverage internal events streaming to expose details about the current workflow execution - while the workflow is running!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2e4e6",
   "metadata": {},
   "source": [
    "## 1. Install needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7601894",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install llama-index-workflows llama-cloud-services llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b991bab",
   "metadata": {},
   "source": [
    "## 2. Define events, workflow state and resources\n",
    "\n",
    "In order for our workflow to work, we will need three things:\n",
    "\n",
    "- Events classes defining the flow\n",
    "- A workflow state representation\n",
    "- External resources to inject into the workflow when needed\n",
    "\n",
    "We will build a workflow that takes a document as input, extracts its raw text content and returns a summary based on that text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275eb8e",
   "metadata": {},
   "source": [
    "### 2.1 Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1468c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows.events import Event, StartEvent, StopEvent\n",
    "\n",
    "\n",
    "class InputDocumentEvent(StartEvent):\n",
    "    document_path: str\n",
    "    summary_prompt: str\n",
    "\n",
    "\n",
    "class ParsedDocumentEvent(Event):\n",
    "    document_content: str\n",
    "\n",
    "\n",
    "class SummaryEvent(StopEvent):\n",
    "    document_summary: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03092946",
   "metadata": {},
   "source": [
    "### 2.2 State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb90c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class WorkflowState(BaseModel):\n",
    "    summary_prompt: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd34ee",
   "metadata": {},
   "source": [
    "### 2.3 Resources\n",
    "\n",
    "For resources, we will use LlamaParse as a document parser, so you will need to set a `LLAMA_CLOUD_API_KEY` in your environment. If you do not have a LlamaCloud API key, you can [get one here](https://cloud.llamaindex.ai).\n",
    "\n",
    "Also, you will need an OpenAI API key to use GPT-5 as a document summarizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edd1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "393a057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cloud_services import LlamaParse\n",
    "from llama_cloud_services.parse import ResultType\n",
    "from llama_index.llms.openai import OpenAIResponses\n",
    "\n",
    "\n",
    "async def get_document_parser(*args, **kwargs) -> LlamaParse:\n",
    "    # we will use LlamaParse in agentic mode\n",
    "    return LlamaParse(\n",
    "        parse_mode=\"parse_page_with_agent\",\n",
    "        model=\"openai-gpt-4-1-mini\",\n",
    "        high_res_ocr=True,\n",
    "        adaptive_long_table=True,\n",
    "        outlined_table_extraction=True,\n",
    "        output_tables_as_HTML=True,\n",
    "        result_type=ResultType.MD,\n",
    "    )\n",
    "\n",
    "\n",
    "async def get_llm_summary(*args, **kwargs) -> OpenAIResponses:\n",
    "    return OpenAIResponses(model=\"gpt-5-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4503f1ae",
   "metadata": {},
   "source": [
    "## 3. Define the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d5d329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from workflows import Workflow, Context, step\n",
    "from workflows.resource import Resource\n",
    "from typing import Annotated\n",
    "\n",
    "\n",
    "class SummaryWorkflow(Workflow):\n",
    "    @step\n",
    "    async def get_document_content(\n",
    "        self,\n",
    "        ev: InputDocumentEvent,\n",
    "        ctx: Context[WorkflowState],\n",
    "        document_parser: Annotated[LlamaParse, Resource(get_document_parser)],\n",
    "    ) -> ParsedDocumentEvent:\n",
    "        async with ctx.store.edit_state() as state:\n",
    "            state.summary_prompt = ev.summary_prompt\n",
    "        result = await document_parser.aparse(ev.document_path)\n",
    "        content = []\n",
    "        if isinstance(result, list):\n",
    "            for r in result:\n",
    "                content.extend((await r.aget_markdown_documents()))\n",
    "        else:\n",
    "            content.extend((await result.aget_markdown_documents()))\n",
    "        text_content = \"\"\n",
    "        for document in content:\n",
    "            text_content += document.text + \"\\n\\n---\\n\\n\"\n",
    "        ctx.write_event_to_stream(ParsedDocumentEvent(document_content=text_content))\n",
    "        return ParsedDocumentEvent(document_content=text_content)\n",
    "\n",
    "    @step\n",
    "    async def summarize_document(\n",
    "        self,\n",
    "        ev: ParsedDocumentEvent,\n",
    "        ctx: Context[WorkflowState],\n",
    "        llm: Annotated[OpenAIResponses, Resource(get_llm_summary)],\n",
    "    ) -> SummaryEvent:\n",
    "        state = await ctx.store.get_state()\n",
    "        summary_prompt = state.summary_prompt\n",
    "        summary_res = await llm.acomplete(\n",
    "            f\"Please create a summary of the following document:\\n\\n'''\\n{ev.document_content}\\n'''\\n\\nFollowing these instructions: {summary_prompt}\"\n",
    "        )\n",
    "        return SummaryEvent(document_summary=summary_res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d0f9fc",
   "metadata": {},
   "source": [
    "## 4. Stream Events\n",
    "\n",
    "In order to stream the internal events, we will pass `expose_internal = True` to the `stream_events` method on the workflow handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11957bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://arxiv.org/pdf/2506.05176 -L -o qwen3_embed_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28ae264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of current step: get_document_content\n",
      "State of current step: running\n",
      "Input event for current step: InputDocumentEvent\n",
      "Output event of current step: No output event yet\n",
      "Started parsing the file under job_id 995119d9-d55f-4872-aa2a-bded8ab3daf9\n",
      "..Document has been successfully parsed!\n",
      "Name of current step: get_document_content\n",
      "State of current step: not_running\n",
      "Input event for current step: <class '__main__.InputDocumentEvent'>\n",
      "Output event of current step: <class '__main__.ParsedDocumentEvent'>\n",
      "Name of current step: summarize_document\n",
      "State of current step: running\n",
      "Input event for current step: ParsedDocumentEvent\n",
      "Output event of current step: No output event yet\n",
      "Name of current step: summarize_document\n",
      "State of current step: not_running\n",
      "Input event for current step: <class '__main__.ParsedDocumentEvent'>\n",
      "Output event of current step: <class '__main__.SummaryEvent'>\n",
      "Here is a concise scientific summary of the Qwen3 Embedding technical report.\n",
      "\n",
      "Overview\n",
      "- The authors introduce the Qwen3 Embedding series: instruction-aware text embedding and point-wise reranking models built on the Qwen3 foundation LLMs. Models are provided at three sizes (0.6B, 4B, 8B parameters) and released under Apache 2.0.\n",
      "- Goals: produce high-quality, multilingual embeddings and rerankers that perform well across retrieval, STS, classification, code retrieval, long documents and instruction-following retrieval tasks.\n",
      "\n",
      "Model design and inference\n",
      "- Backbone: dense (causal-attention) Qwen3 LLMs (0.6B / 4B / 8B), long context (32K), embedding dimensions: 1024 / 2560 / 4096 respectively; embedding models support customizable output dimension (MRL).\n",
      "- Embedding inference: instruction + query placed in input; embedding taken from final-layer hidden state at the appended [EOS] token.\n",
      "- Reranking inference: instruction + query + candidate document presented in a chat template; model produces a binary “yes” / “no” distribution and the relevance score is computed from the probability of “yes”.\n",
      "\n",
      "Training recipe\n",
      "- Multi-stage pipeline for embedding models:\n",
      "  1. Large-scale weakly supervised pre-training on synthetic pair data (∼150M pairs) synthesized by Qwen3-32B with prompts controlling task, language, query type, length, difficulty and persona/role.\n",
      "  2. Supervised fine-tuning on high-quality labeled data (∼7M) combined with filtered synthetic examples (~12M pairs selected by cosine similarity > 0.7).\n",
      "  3. Model merging: spherical linear interpolation (slerp) across multiple fine-tuning checkpoints to improve robustness and generalization.\n",
      "- Reranker training skips the weak-supervision pretraining and uses supervised fine-tuning plus model merging.\n",
      "- Objectives: embeddings optimized with an InfoNCE-style contrastive loss (cosine similarity, temperature, in-batch negatives with a mask to mitigate false negatives). Rerankers optimized with a supervised log-loss on the yes/no label.\n",
      "\n",
      "Synthetic data generation\n",
      "- Two-stage generation per document: (1) configure query type / difficulty / persona via an LLM-assisted selection, (2) generate queries from that configuration. This enables controlled, diverse multilingual synthetic pairs across retrieval, bitext mining, STS and classification tasks.\n",
      "\n",
      "Evaluation and results\n",
      "- Benchmarks: MMTEB (massive multilingual extension of MTEB), MTEB (English), CMTEB (Chinese), MTEB-Code, FollowIR and other retrieval/reranking sets.\n",
      "- Embeddings: Qwen3-Embedding-4B and -8B achieve state-of-the-art scores across MMTEB/MTEB/CMTEB and code retrieval. Representative numbers: Qwen3-Embedding-8B attains 70.58 on the MTEB Multilingual benchmark and 80.68 on MTEB Code (reported comparisons to leading open-source and commercial baselines, including Gemini Embedding).\n",
      "- Rerankers: all Qwen3-Reranker models improve over baseline rerankers and over raw embedding retrieval. The larger rerankers (4B/8B) deliver the best ranking performance; the 8B reranker improves ranking performance by multiple points versus the 0.6B variant (the report cites ~+3.0 points over many tasks).\n",
      "- Practical pipeline: retrieval is performed with embeddings to produce a top-100 list, then rerankers refine ordering; this was used for fair reranker comparisons.\n",
      "\n",
      "Ablations and analysis\n",
      "- Synthetic pretraining is important: training only on synthetic data yields reasonable performance, but removing the weak-supervision stage degrades final results. Example (0.6B model): MMTEB mean(task) drops substantially when synthetic pretraining is removed.\n",
      "- Model merging helps: disabling model merging reduces performance compared to the merged model, confirming merging’s role in robustness/generalization.\n",
      "\n",
      "Conclusions and release\n",
      "- The Qwen3 Embedding series demonstrates that large LLMs can be effectively used both as backbones and as controllable generators of large-scale, high-quality synthetic training data, producing state-of-the-art multilingual and code embeddings and strong rerankers.\n",
      "- Models (0.6B / 4B / 8B for both embedding and reranking) and code are publicly available to encourage reproducibility and community use.\n"
     ]
    }
   ],
   "source": [
    "from workflows.events import StepStateChanged\n",
    "\n",
    "wf = SummaryWorkflow(timeout=600)\n",
    "handler = wf.run(\n",
    "    start_event=InputDocumentEvent(\n",
    "        document_path=\"qwen3_embed_paper.pdf\",\n",
    "        summary_prompt=\"This is a paper, so you should summarize it while still maintaining a scientific tone and its core concepts and findings\",\n",
    "    )\n",
    ")\n",
    "\n",
    "async for event in handler.stream_events(expose_internal=True):\n",
    "    if isinstance(event, StepStateChanged):\n",
    "        print(\"Name of current step:\", event.name)\n",
    "        print(\"State of current step:\", event.step_state.value)\n",
    "        print(\"Input event for current step:\", event.input_event_name)\n",
    "        print(\n",
    "            \"Output event of current step:\",\n",
    "            event.output_event_name or \"No output event yet\",\n",
    "        )\n",
    "    elif isinstance(event, ParsedDocumentEvent):\n",
    "        print(\"Document has been successfully parsed!\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "result = await handler\n",
    "print(result.document_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
